Lang_URL深度采集程序 v0.99 版本



# 配置文件作用说明

------------------------------------------------------------------------------------------------

[User]

whoami = Langzi


[Config]

# 条件设置 & 是与关系。| 是或关系。
# 设置成 None 即不检测存在与否关系，直接保存到本地

# 具体用法看下面例子
title = 浪子&博客
# 标题中必须存在【浪子】和【博客】两个词才允许保存到本地
# 如果设置成None的话，不检测标题关系
# 如果设置title = 浪子
# 标题中必须存在【浪子】才允许保存到本地
# title = None 的话，不检测标题中存不存在词
black_title = 捡到钱|踩到屎
# 标题中出现【捡到钱】或【踩到屎】其中任意一个词就排除这个网址
url = www|cn
# 网址中出现【www】或【cn】其中任意一个词就允许保存到本地
black_url = gov.cn|edu.cn
# 网址中出现【gov.cn】或【edu.cn】其中任意一个词就排除这个网址
content = None
# 不检测网页存在与否关系
# 即不管网页存在什么都保存到本地
black_content = 404|360|安全狗
# 网页中出现【404】或【360】或【安全狗】其中任意一个词就排除这个网址
timeout = 5
# 连接超时5秒
track = 1
# 设置 0 表示对传入的网址不采集友链，直接对传入网址进行动态规则筛选
# 设置 1 将会对传入网址进行友链采集，并且对传入网址和网址的友链进行动态规则筛选
forever = 1
# 对结果重复继续重复爬行友链次数
# 设置 0 表示不爬行，设置1表示无限爬行
white_or = 1 
#设置 white_or = 1 表示所有的白名单(url，title，content中，只要其中一个满足条件就保存到本地，即url = www，title = 国际，content = langzi，只要网址中出现了www就保存到本地)
#设置 white_or = 0 表示所有的白名单(url，title，content中，三个条件都要满足才会保存)
url_thread = 10
# url采集线程数
check_thread = 100
# 规则过滤识别线程数



------------------------------------------------------------------------------------------------


# URL筛选权重比例
# 优先检查 网址黑名单，其次标题黑名单，随后网页黑名单，然后白名单
【** 注意，forever 大于0 的前提条件是track = 1，即必须开启自动爬行友链的前提下才能启用无限采集功能 **】
【** 也就是说，track和forever只要其中一个设置 1 ，另外一个也必须设置1**】
【** 注意，如果不想采集友链不想多次采集的话，设置forever = 0，track =  0**】
【** 注意，凡是 & | None  都是英文输入法 **】
【** 如果你怕你输入错误了  直接在这里复制进去吧~ **】

&&&&&&

||||||

None None None 

